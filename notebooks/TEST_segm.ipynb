{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import TiffFile\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import os\n",
    "import time as t\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construction de x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.6632318496704\n"
     ]
    }
   ],
   "source": [
    "# meme nom dans images et masks\n",
    "files_train = os.listdir(\"./dataset_EI4/train/images\")\n",
    "\n",
    "x_train_liste = []\n",
    "\n",
    "for i in files_train:\n",
    "    with TiffFile('./dataset_EI4/train/images/' + i) as tif:\n",
    "       x_train_liste.append(tif.asarray())\n",
    "\n",
    "\n",
    "x_train = np.array(x_train_liste)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construction de y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.32776737213135\n"
     ]
    }
   ],
   "source": [
    "# meme nom dans images et masks\n",
    "files_train = os.listdir(\"./dataset_EI4/train/images\")\n",
    "\n",
    "y_train_liste = []\n",
    "\n",
    "for i in files_train:\n",
    "\n",
    "    with TiffFile('./dataset_EI4/train/masks/' + i) as tif:\n",
    "        y_train_liste.append(tif.asarray())\n",
    "\n",
    "\n",
    "y_train = np.asarray(y_train_liste)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construction de x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.881664752960205\n"
     ]
    }
   ],
   "source": [
    "# meme nom dans images et masks\n",
    "files_train = os.listdir(\"./dataset_EI4/test/images\")\n",
    "\n",
    "x_test_liste = []\n",
    "\n",
    "for i in files_train:\n",
    "    with TiffFile('./dataset_EI4/test/images/' + i) as tif:\n",
    "       x_test_liste.append(tif.asarray())\n",
    "\n",
    "x_test = np.array(x_test_liste)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construction de y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.299220323562622\n"
     ]
    }
   ],
   "source": [
    "# meme nom dans images et masks\n",
    "files_train = os.listdir(\"./dataset_EI4/test/images\")\n",
    "\n",
    "y_test_liste = []\n",
    "\n",
    "for i in files_train:\n",
    "    with TiffFile('./dataset_EI4/test/masks/' + i) as tif:\n",
    "       y_test_liste.append(tif.asarray())\n",
    "       \n",
    "y_test = np.array(y_test_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18491, 256, 256, 4)\n",
      "(18491, 256, 256)\n",
      "(5043, 256, 256, 4)\n",
      "(5043, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'divide' output from dtype('float64') to dtype('uint16') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#normalisation dans [0,1] reduit la dispersion\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x_train \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m      3\u001b[0m x_test \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m255\u001b[39m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('uint16') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "#normalisation dans [0,1] reduit la dispersion\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de classification segmentée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['nodata', 'clouds', 'artificial surfaces and construction​','cultivated areas​','broadleaf tree cover​','coniferous tree cover​','herbaceous vegetation​','natural material surfaces​','permanent snow-covered surfaces​','water bodies​']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset({'train':})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 18491\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définir le réseau de neurones\n",
    "\n",
    "Pour définir chaque couche de notre réseau de neurones, on va d'abord :\n",
    "- initialiser le modèle avec : `model=tf.keras.models.Sequential()` (c'est un modèle séquentiel i.e. où les couches vont s'enchaîner)\n",
    "- ajouter des couches avec : `model.add(couche)`\n",
    "\n",
    "Les couches qu'on va ajouter :\n",
    "- d'abord une couche `Flatten(input_shape=(im_height,im_width,n_channels))` qui va servir à aplanir l'image en un vecteur. (n_channels= 3 si image en couleur, = 1 si image en noir et blanc)\n",
    " `input_shape=` est un argument obligatoire pour la 1ère couche du modèle, cela détermine la taille des données. Pour les couches suivantes ce n'est plus utile car la taille intermédiaire est calculée automatiquement.\n",
    " - 2 couches `Dense(nb_neurones,activation='à vous de choisir')`, ce sont les couches de neurones intermédiaires (pas besoin de refaire tous les produits matriciels horribles OMG)\n",
    " - une couche de sortie `Dense(n_classes,activation='softmax')`, on prédit un vecteur probabilités de taille `n_classes`. La fonction d'activation softmax permet d'avoir un vecteur de probabilité en sortie.\n",
    "\n",
    " Exemple:\n",
    " ```\n",
    "  model.add(keras.layers.Dense(64,activation='relu'))\n",
    " ```\n",
    " Documentation sur comment faire un modèle séquentiel : https://www.tensorflow.org/guide/keras/sequential_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the neural network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "'''def neural_network():\n",
    "    model=tf.keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(256,256,4)))\n",
    "    model.add(keras.layers.Dense(512,activation='relu'))\n",
    "    model.add(keras.layers.Dense(256,activation='relu'))\n",
    "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "    return model'''\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(256, 256, 4)),\n",
    "    layers.Reshape(target_shape=(256, 256,4)),\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du modele\n",
    "model = neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 262144)            0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               134218240 \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,352,138\n",
      "Trainable params: 134,352,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualisez votre réseau de neurones\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the optimizer,loss function and metrics\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compilation du modèle\n",
    "#model.compile(opt,loss,metrics)\n",
    "#gpt :\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant vous allez entrainer votre modèle sur le dataset qui comporte les images (x_train) associées à leur label (y_train). Pour cela on utilise la méthode `model.fit(...)`.\n",
    "\n",
    "L'argument epochs correspond au nombre de fois où le dataset est présenté au réseau de neurones. Ici mettez entre 1 et 20 epochs.\n",
    "\n",
    "L'argument validation_data sert à vérifier que votre modèle est bon sur des images qu'il n'a jamais vues (pas utilisées dans l'entrainement), i.e. qu'il n'a pas \"appris\" le dataset par cœur.\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 256, 256) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(x_test,y_test),batch_size\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filertjp43dc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\clemm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 256, 256) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=2,validation_data=(x_test,y_test),batch_size=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65a279242d9da0f4c8a8dbec709c60146c0219669dd5920fb12324307df04b3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
